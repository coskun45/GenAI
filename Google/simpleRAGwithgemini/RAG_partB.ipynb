{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilenames\n",
    "def upload_multiple_files():\n",
    "    # Küçük tkinter penceresini gizle\n",
    "    Tk().withdraw()\n",
    "\n",
    "    # Birden fazla dosya seçmek için dosya seçici aç\n",
    "    filenames = askopenfilenames(\n",
    "        title=\"Dosyaları Seçin\",\n",
    "        filetypes=[(\"Tüm Dosyalar\", \"*.*\")]  # İsteğe bağlı olarak dosya türlerini filtreleyebilirsiniz\n",
    "    )\n",
    "\n",
    "    return filenames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=upload_multiple_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(file_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert PDF content to text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf2 --quiet\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_PDF_Text(pdf_path):\n",
    "  reader = PdfReader(pdf_path)\n",
    "  pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
    "  # Filter the empty strings\n",
    "  pdf_texts = [text for text in pdf_texts if text]\n",
    "  print(\"Document: \",pdf_path,\" chunk size: \", len(pdf_texts))\n",
    "  return pdf_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:  C:/Users/ecoskun/Downloads/Bir Müslümanın Yol Haritası - Akademi A. Heyeti.pdf  chunk size:  839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. Giriş\\n2. İman Esasları\\n3. Kâinatı Yaratan Allah (cc)\\n4. Nübüvve t (Peygamberlik Müessesesi)\\n5. İnsanlığa Lütfedilen İlâhî Kitaplar\\n6. Fizik Ötesi/Nuranî Varlıklar: Melekler\\n7. İlâhî Takdir: Kader\\n8. Ölüm Ötesi Sonsuz Hayat: Âhiret\\n9. İbadet Mükellefiyeti\\n10. Namaz\\n11. Oruç\\n12. Zekât\\n13. Hac\\n14. Helâl ve Haramlar\\n15. Temizlik\\n16. Ahlâk\\n17. Kurban'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_texts = convert_PDF_Text(file_names[0])\n",
    "pdf_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> 1. Giriş\n",
       "> 2. İman Esasları\n",
       "> 3. Kâinatı Yaratan Allah (cc)\n",
       "> 4. Nübüvve t (Peygamberlik Müessesesi)\n",
       "> 5. İnsanlığa Lütfedilen İlâhî Kitaplar\n",
       "> 6. Fizik Ötesi/Nuranî Varlıklar: Melekler\n",
       "> 7. İlâhî Takdir: Kader\n",
       "> 8. Ölüm Ötesi Sonsuz Hayat: Âhiret\n",
       "> 9. İbadet Mükellefiyeti\n",
       "> 10. Namaz\n",
       "> 11. Oruç\n",
       "> 12. Zekât\n",
       "> 13. Hac\n",
       "> 14. Helâl ve Haramlar\n",
       "> 15. Temizlik\n",
       "> 16. Ahlâk\n",
       "> 17. Kurban"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "to_markdown(pdf_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/ecoskun/Downloads/Bir Müslümanın Yol Haritası - Akademi A. Heyeti.pdf  has  839  pages\n"
     ]
    }
   ],
   "source": [
    "print(file_names[0],\" has \", len(pdf_texts), \" pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert Text from Pages to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain --quiet\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Page_ChunkinChar(pdf_texts, chunk_size = 1500, chunk_overlap=0 ):\n",
    "  character_splitter = RecursiveCharacterTextSplitter(\n",
    "      separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "      chunk_size=1500,\n",
    "      chunk_overlap=0\n",
    ")\n",
    "  character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "  print(f\"\\nTotal number of chunks (document splited by max char = 1500): \\\n",
    "        {len(character_split_texts)}\")\n",
    "  return character_split_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of chunks (document splited by max char = 1500):         1362\n"
     ]
    }
   ],
   "source": [
    "text_chunksinChar = convert_Page_ChunkinChar(pdf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................. NOTICE ..................\n",
      "C:/Users/ecoskun/Downloads/Bir Müslümanın Yol Haritası - Akademi A. Heyeti.pdf  has  839  pages\n",
      "C:/Users/ecoskun/Downloads/Bir Müslümanın Yol Haritası - Akademi A. Heyeti.pdf  has  1362  chunks\n",
      "chunk [0] has  540  chars\n",
      "chunk [-1] has  69  chars\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"................. NOTICE ..................\")\n",
    "print(file_names[0],\" has \", len(pdf_texts), \" pages\")\n",
    "print(file_names[0],\" has \", len(text_chunksinChar), \" chunks\")\n",
    "print(\"chunk [0] has \", len(text_chunksinChar[0]), \" chars\")\n",
    "print(\"chunk [-1] has \", len(text_chunksinChar[-1]), \" chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert Text from Chunks to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text from Chunks to Tokens\n",
    "\n",
    "This code is designed to tokenize the text chunks produced by the previous code using the `SentenceTransformersTokenTextSplitter`. Let's break down the code and its purpose:\n",
    "\n",
    "1. `token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, model_name=\"distiluse-base-multilingual-cased-v1\", tokens_per_chunk=128)`: This line initializes a tokenizer object named `token_splitter`. It utilizes the `SentenceTransformersTokenTextSplitter` class, which tokenizes text into chunks based on a specified model. The parameters provided are:\n",
    "   - `chunk_overlap=0`: This parameter specifies that there should be no overlap between tokenized chunks.\n",
    "   - `model_name=\"distiluse-base-multilingual-cased-v1\"`: This parameter specifies the name of the pre-trained model to be used for tokenization. In this case, it's using the \"distiluse-base-multilingual-cased-v1\" model.\n",
    "\n",
    "   - `tokens_per_chunk=128`: This parameter determines the maximum number of tokens allowed per chunk after tokenization.\n",
    "\n",
    "2. `token_split_texts = []`: This line initializes an empty list to store the tokenized text chunks.\n",
    "\n",
    "3. `for text in character_split_texts:`: This line starts a loop iterating over the text chunks produced by the character splitter.\n",
    "\n",
    "4. `token_split_texts += token_splitter.split_text(text)`: Within the loop, this line tokenizes each text chunk using the `token_splitter` object and adds the resulting tokenized chunks to the `token_split_texts` list.\n",
    "\n",
    "5. `print(token_split_texts[10])`: This line prints out the 11th tokenized chunk for inspection.\n",
    "\n",
    "6. `print(f\"\\nTotal chunks: {len(token_split_texts)}\")`: This line prints out the total number of tokenized chunks produced.\n",
    "\n",
    "The reason for applying this code after the character splitting code is likely to further process the text data. Character splitting breaks down the text into smaller, more manageable chunks, and tokenization further breaks down those chunks into individual tokens, often for tasks like natural language processing (NLP) or machine learning. By applying both character splitting and tokenization, the text data becomes more structured and suitable for various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers --quiet\n",
    "!pip install tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "sentence_transformer_model=\"distiluse-base-multilingual-cased-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Chunk_Token(text_chunksinChar,sentence_transformer_model, chunk_overlap=0,tokens_per_chunk=128 ):\n",
    "  token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "      chunk_overlap=chunk_overlap,\n",
    "      model_name=sentence_transformer_model,\n",
    "      tokens_per_chunk=tokens_per_chunk)\n",
    "\n",
    "  text_chunksinTokens = []\n",
    "  for text in text_chunksinChar:\n",
    "      text_chunksinTokens += token_splitter.split_text(text)\n",
    "  print(f\"\\nTotal number of chunks (document splited by 128 tokens per chunk):\\\n",
    "       {len(text_chunksinTokens)}\")\n",
    "  return text_chunksinTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text_chunksinTokens \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_Chunk_Token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_chunksinChar\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentence_transformer_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m, in \u001b[0;36mconvert_Chunk_Token\u001b[1;34m(text_chunksinChar, sentence_transformer_model, chunk_overlap, tokens_per_chunk)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_Chunk_Token\u001b[39m(text_chunksinChar,sentence_transformer_model, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,tokens_per_chunk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m ):\n\u001b[1;32m----> 2\u001b[0m   token_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformersTokenTextSplitter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_transformer_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtokens_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_per_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m   text_chunksinTokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m text_chunksinChar:\n",
      "File \u001b[1;32mc:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_text_splitters\\sentence_transformers.py:22\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.__init__\u001b[1;34m(self, chunk_overlap, model_name, tokens_per_chunk, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, chunk_overlap\u001b[38;5;241m=\u001b[39mchunk_overlap)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformer python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to for SentenceTransformersTokenTextSplitter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[1;32mc:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Dict, List, Literal, Optional, Tuple, Type, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n",
      "File \u001b[1;32mc:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "text_chunksinTokens = convert_Chunk_Token(text_chunksinChar,sentence_transformer_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
