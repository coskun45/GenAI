{"cells":[{"cell_type":"markdown","metadata":{"id":"spB6iVzXxT1p"},"source":["# PART A: AN INTRO TO GEMINI API FOR TEXT GENERATION & CHAT\n","\n","\n","RAG stands for Retrieval-Augmented Generation. It's a technique that combines large language models (LLMs) with external knowledge sources to improve the accuracy and reliability of AI-generated text.\n","\n","## How Does RAG Work? Unveiling the Power of External Knowledge\n","\n","Before we start the core RAG process, we need to provide a foundation as follows:\n","\n","* **Building the Knowledge Base:** The system starts by transforming documents and information within the external knowledge base (like Wikipedia or a company database) into a special format called **vector representations**. These condense the meaning of each document into a series of **numbers**, capturing the essence of the content.\n","\n","* **Vector Database for Speedy Retrieval**: These vector representations are then stored in a specialized database called a vector database. This database is optimized for efficiently **searching and retrieving** information based on **semantic similarity**. Imagine it as a super-powered library catalog that **understands the meaning** of documents, **not just keywords**.\n","\n","Now, let's explore how RAG leverages this foundation:\n","\n","* **User Input**: The RAG process begins with a question or **prompt** from the user. This could be anything from \"What caused the extinction of the dinosaurs?\" to a more open-ended request like \"Write a creative story.\"\n","\n","* **Intelligent Retrieval**: RAG doesn't rely solely on the **LLM's internal knowledge**. It employs an information retrieval component that acts like a super-powered search engine. This component scans the vast external knowledge base – like a company's internal database for specific domains – to find information **directly relevant** to the user's input. Unlike a traditional **search engine** that relies on **keywords**, RAG leverages the power of vector representations to understand the **semantic meaning** of the user's prompt and identify the most relevant documents.\n","\n","* **Enriched Context Creation**: The retrieved information isn't just shown alongside the prompt. RAG cleverly **merges the user input with the relevant snippets** from the knowledge base. This creates a ***richer context*** for the LLM to understand the **user's intent** and formulate a well-informed response.\n","\n","* **LLM Powered Response Generation**: Finally, the **enriched context** is fed to the Large Language Model (LLM). The LLM, along with its ability to process language patterns, now has a strong **foundation of factual** information to draw upon. This empowers it to generate a response that is both comprehensive and accurate, addressing the specific needs of the user's prompt.\n","\n","In this part, we will learn how provide an LLM connection and generate text using Google Gemini API.\n","\n","https://ai.google.dev/gemini-api/docs\n","\n","## CONTENT\n","* The Python SDK for the Gemini API\n","* Check the Google LLM Models available via the provided API\n","* Interact with the models using 2 Alternative Interfaces\n","  1. Generate text interface\n","  2. Interact with the models using Multi-turn conversations (chat) interface\n","\n","* Understand Model & Chat objects\n","  * Model Object in detail\n","  * System Prompt in the Gemini API\n","  * Chat Object in detail\n","* Chat using system_instruction: ***A Manual RAG?***\n","* How Many Tokens --> How much does it cost?\n","* Build a simple Interface with Gradio\n"]},{"cell_type":"markdown","metadata":{"id":"beMCPjuLLhAM"},"source":["## Install the Python SDK\n","\n","* The Python SDK for the Gemini API, is contained in the [`google-generativeai`](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"E607BT94Lb3F"},"outputs":[],"source":["!pip install -q -U google-generativeai"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"7wZWDPDCJina"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["#import numpy as np\n","#from tqdm import tqdm\n","#import pathlib\n","import os\n","import textwrap\n","import google.generativeai as genai\n","from IPython.display import display\n","from IPython.display import Markdown"]},{"cell_type":"markdown","metadata":{"id":"9R_7tVjp-hne"},"source":["* The **to_markdown** function converts plain text from the LLM model to Markdown format, adding blockquote styling and converting bullet points."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Zfy7rLWNLt5W"},"outputs":[],"source":["def to_markdown(text):\n","  text = text.replace('•', '  *')\n","  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"k2rzs85R7iYf"},"outputs":[],"source":["from dotenv import load_dotenv\n","import os\n","load_dotenv()\n","api_key = os.getenv(\"GOOGLE_API_KEY\")\n","genai.configure(api_key=api_key)"]},{"cell_type":"markdown","metadata":{"id":"9ztqhxys_OdX"},"source":["## Check the Google LLM Models available via the provided API\n","\n","* You can see the names of the available models as follows:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"elapsed":2558,"status":"ok","timestamp":1717527388120,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"qU7wtu2e55z9","outputId":"318901b6-7d15-405f-ab79-8f13432b9868"},"outputs":[{"name":"stdout","output_type":"stream","text":["models/gemini-1.0-pro-latest\n","models/gemini-1.0-pro\n","models/gemini-pro\n","models/gemini-1.0-pro-001\n","models/gemini-1.0-pro-vision-latest\n","models/gemini-pro-vision\n","models/gemini-1.5-pro-latest\n","models/gemini-1.5-pro-001\n","models/gemini-1.5-pro\n","models/gemini-1.5-pro-exp-0801\n","models/gemini-1.5-flash-latest\n","models/gemini-1.5-flash-001\n","models/gemini-1.5-flash\n","models/gemini-1.5-flash-001-tuning\n"]}],"source":["for m in genai.list_models():\n","  if 'generateContent' in m.supported_generation_methods:\n","    print(m.name)"]},{"cell_type":"markdown","metadata":{"id":"i6gnd-fR_08c"},"source":["* You can see the details of the models as follows:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1605,"status":"ok","timestamp":1717527389710,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"CbDIFnIA7oZ5","outputId":"3e69ebdc-abd3-4ee9-8d92-1d647c98a479"},"outputs":[{"data":{"text/plain":["[Model(name='models/chat-bison-001',\n","       base_model_id='',\n","       version='001',\n","       display_name='PaLM 2 Chat (Legacy)',\n","       description='A legacy text-only model optimized for chat conversations',\n","       input_token_limit=4096,\n","       output_token_limit=1024,\n","       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n","       temperature=0.25,\n","       max_temperature=None,\n","       top_p=0.95,\n","       top_k=40),\n"," Model(name='models/text-bison-001',\n","       base_model_id='',\n","       version='001',\n","       display_name='PaLM 2 (Legacy)',\n","       description='A legacy model that understands text and generates text as an output',\n","       input_token_limit=8196,\n","       output_token_limit=1024,\n","       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n","       temperature=0.7,\n","       max_temperature=None,\n","       top_p=0.95,\n","       top_k=40),\n"," Model(name='models/embedding-gecko-001',\n","       base_model_id='',\n","       version='001',\n","       display_name='Embedding Gecko',\n","       description='Obtain a distributed representation of a text.',\n","       input_token_limit=1024,\n","       output_token_limit=1,\n","       supported_generation_methods=['embedText', 'countTextTokens'],\n","       temperature=None,\n","       max_temperature=None,\n","       top_p=None,\n","       top_k=None),\n"," Model(name='models/gemini-1.0-pro-latest',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.0 Pro Latest',\n","       description=('The best model for scaling across a wide range of tasks. This is the latest '\n","                    'model.'),\n","       input_token_limit=30720,\n","       output_token_limit=2048,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=0.9,\n","       max_temperature=None,\n","       top_p=1.0,\n","       top_k=None),\n"," Model(name='models/gemini-1.0-pro',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.0 Pro',\n","       description='The best model for scaling across a wide range of tasks',\n","       input_token_limit=30720,\n","       output_token_limit=2048,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=0.9,\n","       max_temperature=None,\n","       top_p=1.0,\n","       top_k=None),\n"," Model(name='models/gemini-pro',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.0 Pro',\n","       description='The best model for scaling across a wide range of tasks',\n","       input_token_limit=30720,\n","       output_token_limit=2048,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=0.9,\n","       max_temperature=None,\n","       top_p=1.0,\n","       top_k=None),\n"," Model(name='models/gemini-1.0-pro-001',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.0 Pro 001 (Tuning)',\n","       description=('The best model for scaling across a wide range of tasks. This is a stable '\n","                    'model that supports tuning.'),\n","       input_token_limit=30720,\n","       output_token_limit=2048,\n","       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n","       temperature=0.9,\n","       max_temperature=None,\n","       top_p=1.0,\n","       top_k=None),\n"," Model(name='models/gemini-1.0-pro-vision-latest',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.0 Pro Vision',\n","       description='The best image understanding model to handle a broad range of applications',\n","       input_token_limit=12288,\n","       output_token_limit=4096,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=0.4,\n","       max_temperature=None,\n","       top_p=1.0,\n","       top_k=32),\n"," Model(name='models/gemini-pro-vision',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.0 Pro Vision',\n","       description='The best image understanding model to handle a broad range of applications',\n","       input_token_limit=12288,\n","       output_token_limit=4096,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=0.4,\n","       max_temperature=None,\n","       top_p=1.0,\n","       top_k=32),\n"," Model(name='models/gemini-1.5-pro-latest',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.5 Pro Latest',\n","       description='Mid-size multimodal model that supports up to 2 million tokens',\n","       input_token_limit=2097152,\n","       output_token_limit=8192,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=1.0,\n","       max_temperature=2.0,\n","       top_p=0.95,\n","       top_k=64),\n"," Model(name='models/gemini-1.5-pro-001',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.5 Pro 001',\n","       description='Mid-size multimodal model that supports up to 2 million tokens',\n","       input_token_limit=2097152,\n","       output_token_limit=8192,\n","       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n","       temperature=1.0,\n","       max_temperature=2.0,\n","       top_p=0.95,\n","       top_k=64),\n"," Model(name='models/gemini-1.5-pro',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.5 Pro',\n","       description='Mid-size multimodal model that supports up to 2 million tokens',\n","       input_token_limit=2097152,\n","       output_token_limit=8192,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=1.0,\n","       max_temperature=2.0,\n","       top_p=0.95,\n","       top_k=64),\n"," Model(name='models/gemini-1.5-pro-exp-0801',\n","       base_model_id='',\n","       version='exp-0801',\n","       display_name='Gemini 1.5 Pro Experimental 0801',\n","       description='Mid-size multimodal model that supports up to 2 million tokens',\n","       input_token_limit=2097152,\n","       output_token_limit=8192,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=1.0,\n","       max_temperature=2.0,\n","       top_p=0.95,\n","       top_k=64),\n"," Model(name='models/gemini-1.5-flash-latest',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.5 Flash Latest',\n","       description='Fast and versatile multimodal model for scaling across diverse tasks',\n","       input_token_limit=1048576,\n","       output_token_limit=8192,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=1.0,\n","       max_temperature=2.0,\n","       top_p=0.95,\n","       top_k=64),\n"," Model(name='models/gemini-1.5-flash-001',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.5 Flash 001',\n","       description='Fast and versatile multimodal model for scaling across diverse tasks',\n","       input_token_limit=1048576,\n","       output_token_limit=8192,\n","       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n","       temperature=1.0,\n","       max_temperature=2.0,\n","       top_p=0.95,\n","       top_k=64),\n"," Model(name='models/gemini-1.5-flash',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.5 Flash',\n","       description='Fast and versatile multimodal model for scaling across diverse tasks',\n","       input_token_limit=1048576,\n","       output_token_limit=8192,\n","       supported_generation_methods=['generateContent', 'countTokens'],\n","       temperature=1.0,\n","       max_temperature=2.0,\n","       top_p=0.95,\n","       top_k=64),\n"," Model(name='models/gemini-1.5-flash-001-tuning',\n","       base_model_id='',\n","       version='001',\n","       display_name='Gemini 1.5 Flash 001 Tuning',\n","       description='Fast and versatile multimodal model for scaling across diverse tasks',\n","       input_token_limit=16384,\n","       output_token_limit=8192,\n","       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n","       temperature=1.0,\n","       max_temperature=2.0,\n","       top_p=0.95,\n","       top_k=64),\n"," Model(name='models/embedding-001',\n","       base_model_id='',\n","       version='001',\n","       display_name='Embedding 001',\n","       description='Obtain a distributed representation of a text.',\n","       input_token_limit=2048,\n","       output_token_limit=1,\n","       supported_generation_methods=['embedContent'],\n","       temperature=None,\n","       max_temperature=None,\n","       top_p=None,\n","       top_k=None),\n"," Model(name='models/text-embedding-004',\n","       base_model_id='',\n","       version='004',\n","       display_name='Text Embedding 004',\n","       description='Obtain a distributed representation of a text.',\n","       input_token_limit=2048,\n","       output_token_limit=1,\n","       supported_generation_methods=['embedContent'],\n","       temperature=None,\n","       max_temperature=None,\n","       top_p=None,\n","       top_k=None),\n"," Model(name='models/aqa',\n","       base_model_id='',\n","       version='001',\n","       display_name='Model that performs Attributed Question Answering.',\n","       description=('Model trained to return answers to questions that are grounded in provided '\n","                    'sources, along with estimating answerable probability.'),\n","       input_token_limit=7168,\n","       output_token_limit=1024,\n","       supported_generation_methods=['generateAnswer'],\n","       temperature=0.2,\n","       max_temperature=None,\n","       top_p=1.0,\n","       top_k=40)]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["models = [m for m in genai.list_models()]\n","models"]},{"cell_type":"markdown","metadata":{"id":"IwvOrH0YR6mr"},"source":["## Interact with the models using 2 Alternative Interfaces\n","\n","1. Generate text\n","2. Multi-turn conversations (chat)"]},{"cell_type":"markdown","metadata":{"id":"hz2qZhHwSWfU"},"source":["## 1. Generate text interface\n","\n","In the simplest case, you can pass a prompt string to the GenerativeModel.generate_content method:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"elapsed":4982,"status":"ok","timestamp":1717532624447,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"CYOPquLVSjLM","outputId":"e0a7202e-f423-43bc-92fe-2cef237286f3"},"outputs":[{"data":{"text/markdown":["> There is **one** primary way to access models in the Gemini API, which is through **REST API calls**.\n","> \n","> However, within this method, there are several variations and parameters you can use to customize your requests:\n","> \n","> * **Different Endpoints:** Depending on your desired task, you can choose from various endpoints for generating text, translating languages, writing different kinds of creative content, and more.\n","> * **Model Selection:** You have the option to specify the desired Gemini model (e.g., Gemini Pro, Gemini Ultra) within your API calls.\n","> * **Request Parameters:** You can tailor your requests with parameters like temperature, top_k, top_p, and others to control the output's creativity, randomness, and more.\n","> * **Fine-tuning Options:** Although not yet available for Gemini, the API might eventually offer features for fine-tuning models for specific tasks or domains.\n","> \n","> Therefore, while there's only **one** fundamental access method (REST API), the possibilities within this method are vast and allow you to interact with the models in diverse ways. \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["model = genai.GenerativeModel('gemini-1.5-flash-latest')\n","response = model.generate_content(\"How many different ways to acccess a model in Gemini API?\")\n","to_markdown(response.text)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1717532719121,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"dOoJBSzgUIGI","outputId":"fe440978-e0e0-4869-dfb4-b12498cb7582"},"outputs":[{"data":{"text/plain":["response:\n","GenerateContentResponse(\n","    done=True,\n","    iterator=None,\n","    result=protos.GenerateContentResponse({\n","      \"candidates\": [\n","        {\n","          \"content\": {\n","            \"parts\": [\n","              {\n","                \"text\": \"There is **one** primary way to access models in the Gemini API, which is through **REST API calls**.\\n\\nHowever, within this method, there are several variations and parameters you can use to customize your requests:\\n\\n* **Different Endpoints:** Depending on your desired task, you can choose from various endpoints for generating text, translating languages, writing different kinds of creative content, and more.\\n* **Model Selection:** You have the option to specify the desired Gemini model (e.g., Gemini Pro, Gemini Ultra) within your API calls.\\n* **Request Parameters:** You can tailor your requests with parameters like temperature, top_k, top_p, and others to control the output's creativity, randomness, and more.\\n* **Fine-tuning Options:** Although not yet available for Gemini, the API might eventually offer features for fine-tuning models for specific tasks or domains.\\n\\nTherefore, while there's only **one** fundamental access method (REST API), the possibilities within this method are vast and allow you to interact with the models in diverse ways. \\n\"\n","              }\n","            ],\n","            \"role\": \"model\"\n","          },\n","          \"finish_reason\": \"STOP\",\n","          \"index\": 0,\n","          \"safety_ratings\": [\n","            {\n","              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n","              \"probability\": \"NEGLIGIBLE\"\n","            },\n","            {\n","              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n","              \"probability\": \"NEGLIGIBLE\"\n","            },\n","            {\n","              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n","              \"probability\": \"NEGLIGIBLE\"\n","            },\n","            {\n","              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n","              \"probability\": \"NEGLIGIBLE\"\n","            }\n","          ]\n","        }\n","      ],\n","      \"usage_metadata\": {\n","        \"prompt_token_count\": 14,\n","        \"candidates_token_count\": 220,\n","        \"total_token_count\": 234\n","      }\n","    }),\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["response"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":185},"executionInfo":{"elapsed":4744,"status":"ok","timestamp":1717532850246,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"4UQhtWIsUDEX","outputId":"2f5bafc6-d645-4940-a0e6-ca389ff67a41"},"outputs":[{"data":{"text/markdown":["> Please provide me with the context of our conversation. I need to know what you asked me before I can tell you what API you used. For example, did you ask me to:\n","> \n","> * Generate text?\n","> * Translate a language?\n","> * Summarize a text?\n","> * Answer a question? \n","> \n","> Once you provide me with this information, I can tell you which API I used to fulfill your request. \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["response = model.generate_content(\"Which API did I ask you?\")\n","to_markdown(response.text)"]},{"cell_type":"markdown","metadata":{"id":"VBaNTOOF_6xD"},"source":["## 2. Interact with the models using Multi-turn conversations (chat) interface\n","\n","* This code snippet initializes a Gemini AI model and starts a chat session  with an empty conversation history."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"elapsed":5992,"status":"ok","timestamp":1717532948175,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"uE4sH55ABY-e","outputId":"26311758-3aff-4191-f98b-d9ed8b663b48"},"outputs":[{"data":{"text/markdown":["> Unfortunately, there isn't a single, definitive answer to how many ways you can access a Gemini model through the API. This is because Google hasn't publicly released specific details about the Gemini API, including its structure and available methods. \n","> \n","> **However, we can speculate based on how Google typically handles APIs and common practices:**\n","> \n","> * **REST API:** This is the most common approach for APIs, offering a standardized way to interact with resources using HTTP methods (GET, POST, PUT, DELETE, etc.).\n","> * **gRPC:** Google often uses gRPC for efficient communication between services. This would provide high-performance access to the Gemini models.\n","> * **Cloud Functions:** Google might offer integration with Cloud Functions, enabling you to directly trigger model requests from your own serverless code.\n","> * **SDKs:**  Google might provide official SDKs for various programming languages, simplifying interaction with the API.\n","> \n","> **In summary, the number of ways to access a Gemini model through the API is likely to be multiple, leveraging different techniques for different use cases and user preferences. However, until Google officially releases the Gemini API documentation, the exact methods and their details remain unknown.**\n","> \n","> **Keep an eye out for official announcements from Google regarding the Gemini API release, as that will provide the most accurate and up-to-date information.** \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model = genai.GenerativeModel('gemini-1.5-flash-latest')\n","#response = model.generate_content(\"How many different ways to acccess a model in the Gemini API?\")\n","chat = model.start_chat(history=[])\n","response = chat.send_message(\"How many different ways to acccess a model in the Gemini API?\")\n","to_markdown(response.text)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"executionInfo":{"elapsed":3407,"status":"ok","timestamp":1717532966804,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"Kkt7RKG6VR8t","outputId":"b999172d-7501-4eee-e119-f1a9a6b7297b"},"outputs":[{"data":{"text/markdown":["> You asked me about the **Gemini API**.  \n","> \n","> This is the API for accessing Google's Gemini models, which are a powerful family of large language models known for their advanced capabilities. \n","> \n","> Did you have a specific question about the Gemini API, or were you curious about how many different ways you might interact with it? \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#response = model.generate_content(\"Which API did I ask you?\")\n","response =chat.send_message(\"Which API did I ask you?\")\n","to_markdown(response.text)"]},{"cell_type":"markdown","metadata":{"id":"q1Z667a7uEi9"},"source":["## Understand Model & Chat objects"]},{"cell_type":"markdown","metadata":{"id":"u7jTl8E3I1kW"},"source":["* Let's check the created **model** object first, and then the **chat** object:"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":449,"status":"ok","timestamp":1717532987803,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"-9iGxcLJGE0A","outputId":"383ef460-7c97-48a9-ed1d-7942c94fdd3a"},"outputs":[{"data":{"text/plain":["genai.GenerativeModel(\n","    model_name='models/gemini-1.5-flash-latest',\n","    generation_config={},\n","    safety_settings={},\n","    tools=None,\n","    system_instruction=None,\n","    cached_content=None\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"0of0_YMEJDLr"},"source":["* genai.GenerativeModel(...): This creates a Gemini model object for interacting with the API.\n","\n","* model_name='models/gemini-1.5-flash-latest': This specifies which Gemini model version to use. Here, it's \"gemini-1.5-flash-latest\", a powerful model known for its capabilities.\n","* generation_config={}: This is a dictionary for customizing how the model generates text. The empty braces {} mean you're using default generation settings.\n","* safety_settings={}: This is for configuring safety features, like preventing harmful or inappropriate responses. Empty braces again mean you're using default settings.\n","* tools=None: This part is for integrating external tools with the model (e.g., accessing information from a database). Since it's None, no external tools are being used.\n","* **system_instruction=None:** This is similar to a \"system prompt\" in other models, but Gemini API doesn't directly support system prompts. This instruction might have some influence on the model's behavior, but it's not a standard system prompt feature."]},{"cell_type":"markdown","metadata":{"id":"3CARVemnAz9y"},"source":["## System Prompt in the Gemini API:\n","\n","**What is a System Prompt?**\n","\n","In models like ChatGPT, a system prompt is a special instruction provided at the start of a conversation. It helps define the persona, tone, or overall purpose of the model's responses.\n","\n","\n","Unfortunately, the Gemini API **does not** offer a concept directly equivalent to a \"system prompt\" as found in other large language models like ChatGPT.\n","\n","\n","**How Gemini API Works**\n","\n","The Gemini API functions differently. It prioritizes a task-oriented approach, focusing on generating responses based on specific instructions and context provided through its API calls.\n","\n","**Alternatives for Defining Behavior:**\n","\n","While a dedicated system prompt is absent, you can achieve similar effects through these methods:\n","* Prompt Engineering: Craft your API requests with clear and concise instructions, including desired tone, format, or limitations.\n","* Contextualization: Provide relevant information and examples within your API call to guide Gemini's responses.\n","Model Variants: Gemini API offers various model sizes. Choosing a specific size might align with your desired behavior (e.g., a larger model for more comprehensive responses)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"bptkaqzV6AkD"},"outputs":[],"source":["system_prompt= \"\"\" As an attentive and supportive academic assistant,\n","           your task is to provide assistance based solely on the provided\n","           excerpts. I will provide you the question and related text.\n","           Answer the following questions, ensuring your responses\n","           are derived exclusively from the provided partial texts.\n","           If the answer cannot be found within the provided excerpts,\n","           kindly respond with 'I don't know'.\n","           After answering each question, please provide a detailed\n","           explanation, breaking down the answer step by step and relating\n","           it to the provided excerpts.\n","           If you are ready, I will provide you the question and related text.\n","        \"\"\""]},{"cell_type":"code","execution_count":14,"metadata":{"id":"UJ3uzuLkGgDZ"},"outputs":[],"source":["model = genai.GenerativeModel('gemini-1.5-flash-latest', system_instruction=system_prompt)\n","chat = model.start_chat(history=[])"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1717533254243,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"olt2Et5OGkr3","outputId":"5655bfd5-6a0f-41a4-9797-0daa5c91734e"},"outputs":[{"data":{"text/plain":["genai.GenerativeModel(\n","    model_name='models/gemini-1.5-flash-latest',\n","    generation_config={},\n","    safety_settings={},\n","    tools=None,\n","    system_instruction=\" As an attentive and supportive academic assistant,\\n           your task is to provide assistance based solely on the provided\\n           excerpts. I will provide you the question and related text.\\n           Answer the following questions, ensuring your responses\\n           are derived exclusively from the provided partial texts.\\n           If the answer cannot be found within the provided excerpts,\\n           kindly respond with 'I don't know'.\\n           After answering each question, please provide a detailed\\n           explanation, breaking down the answer step by step and relating\\n           it to the provided excerpts.\\n           If you are ready, I will provide you the question and related text.\\n        \",\n","    cached_content=None\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"aPUF-yNsCKcJ"},"source":["## Does system_instruction work as system_prompt?\n","\n","Let's check:\n","* This code snippet interacts with the Gemini chat session we initiated above.\n","1. Sends your question/prompt to the Gemini chat.\n","2. Times how long it takes Gemini to respond.\n","3. Formats the Gemini's response into Markdown for cleaner display."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"executionInfo":{"elapsed":5331,"status":"ok","timestamp":1717533327878,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"qTTko3uuCDFW","outputId":"5a7c458c-cdc1-4688-bf2d-7f39929fb702"},"outputs":[{"data":{"text/markdown":["> My task is to act as an attentive and supportive academic assistant. I will use the provided text excerpts to answer your questions.  \n","> \n","> * I will only use the information in the given text excerpts to answer your questions.\n","> * If the answer cannot be found in the provided excerpts, I will respond with \"I don't know.\" \n","> * I will provide a detailed explanation for each answer, showing how I arrived at the answer based on the text excerpts.\n","> \n","> Please provide me with the question and related text. I am ready to assist you. \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["prompt=\"What is your task? \"\n","response = chat.send_message(prompt)\n","to_markdown(response.text)"]},{"cell_type":"markdown","metadata":{"id":"q6WJEfbnLaAB"},"source":["## Chat Object in detail\n","* Let's observe the **chat** object:"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1717533404781,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"vgYA8E78Fhon","outputId":"ad5a4849-745f-4878-c098-85fe324c47c1"},"outputs":[{"data":{"text/plain":["ChatSession(\n","    model=genai.GenerativeModel(\n","        model_name='models/gemini-1.5-flash-latest',\n","        generation_config={},\n","        safety_settings={},\n","        tools=None,\n","        system_instruction=\" As an attentive and supportive academic assistant,\\n           your task is to provide assistance based solely on the provided\\n           excerpts. I will provide you the question and related text.\\n           Answer the following questions, ensuring your responses\\n           are derived exclusively from the provided partial texts.\\n           If the answer cannot be found within the provided excerpts,\\n           kindly respond with 'I don't know'.\\n           After answering each question, please provide a detailed\\n           explanation, breaking down the answer step by step and relating\\n           it to the provided excerpts.\\n           If you are ready, I will provide you the question and related text.\\n        \",\n","        cached_content=None\n","    ),\n","    history=[protos.Content({'parts': [{'text': 'What is your task? '}], 'role': 'user'}), protos.Content({'parts': [{'text': 'My task is t...ssist you. \\n'}], 'role': 'model'})]\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["chat"]},{"cell_type":"markdown","metadata":{"id":"F2jAKnuGLl4f"},"source":["* We can access the chat history:"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":342,"status":"ok","timestamp":1717533468527,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"eLBFVENxZ2zh","outputId":"3aca5ec3-0ca1-4494-e985-d0d88aab5eef"},"outputs":[{"data":{"text/plain":["[parts {\n","   text: \"What is your task? \"\n"," }\n"," role: \"user\",\n"," parts {\n","   text: \"My task is to act as an attentive and supportive academic assistant. I will use the provided text excerpts to answer your questions.  \\n\\n* I will only use the information in the given text excerpts to answer your questions.\\n* If the answer cannot be found in the provided excerpts, I will respond with \\\"I don\\'t know.\\\" \\n* I will provide a detailed explanation for each answer, showing how I arrived at the answer based on the text excerpts.\\n\\nPlease provide me with the question and related text. I am ready to assist you. \\n\"\n"," }\n"," role: \"model\"]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["chat.history"]},{"cell_type":"markdown","metadata":{"id":"m-u8Xi9EMWfh"},"source":["* Let's see the **chat history** in a bit formatted way:"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"lOV63wmDX9V-"},"outputs":[],"source":["def printChatHistory():\n","  for message in chat.history:\n","    display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n","    display('_'*80)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"elapsed":667,"status":"ok","timestamp":1717533502961,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"TyqP2ix_PCoC","outputId":"8ca0b208-14ae-437f-999a-e77bae8c8efe"},"outputs":[{"data":{"text/markdown":["> **user**: What is your task? "],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'________________________________________________________________________________'"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["> **model**: My task is to act as an attentive and supportive academic assistant. I will use the provided text excerpts to answer your questions.  \n","> \n","> * I will only use the information in the given text excerpts to answer your questions.\n","> * If the answer cannot be found in the provided excerpts, I will respond with \"I don't know.\" \n","> * I will provide a detailed explanation for each answer, showing how I arrived at the answer based on the text excerpts.\n","> \n","> Please provide me with the question and related text. I am ready to assist you. \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'________________________________________________________________________________'"]},"metadata":{},"output_type":"display_data"}],"source":["printChatHistory()"]},{"cell_type":"markdown","metadata":{"id":"rRhmfhVRLvJw"},"source":["# Let's chat according to the system_instruction\n","\n","* Remember the system_instruction\n","* Our aim is to build a **RAG** pipeline for the future tutorials\n","* Therefore, here, we provide some text and a question relat5ed to the text"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"elapsed":6296,"status":"ok","timestamp":1717533606179,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"RLR17eeodoaI","outputId":"f83a13c7-de0c-4ba7-bdad-1a328849dc41"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 0 ns\n","Wall time: 1.65 s\n"]},{"data":{"text/markdown":["> The provided text highlights the difference between the \"chat\" and \"generate_content\" functionalities within the Gemini framework. Here's a breakdown:\n","> \n","> * **Chat:** This functionality allows for free-flowing conversations across multiple turns. The `ChatSession` class handles the conversation's state, meaning you don't need to manually manage the history of the conversation. This makes it easier to have natural, back-and-forth exchanges. \n","> * **Generate Content:** This functionality seems to refer to a more static form of content creation. The text implies that with `generate_content`, you would need to store the conversation history as a list, suggesting that it doesn't maintain a conversational state. \n","> \n","> **In summary:**\n","> \n","> The key difference is in how the conversation history is managed.  `Chat` uses the `ChatSession` class to automatically track the conversation state, making it suitable for multi-turn, dynamic interactions. On the other hand, `generate_content` appears to require manual management of conversation history, indicating a more static approach to text generation. \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","question= \"What is the difference between chat and generate context?\"\n","excerpt= \"\"\" Gemini enables you to have freeform conversations across\n","multiple turns. The ChatSession class simplifies the process\n","by managing the state of the conversation, so unlike with\n","generate_content, you do not have to store the conversation\n","history as a list.\n","\"\"\"\n","prompt = question + excerpt\n","response = chat.send_message(prompt)\n","\n","to_markdown(response.text)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"elapsed":9174,"status":"ok","timestamp":1717533660489,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"8hvmm8KEYG_q","outputId":"07e4e220-7d23-44ca-9a20-6ad0a486507d"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 31.2 ms\n","Wall time: 1.61 s\n"]},{"data":{"text/markdown":["> The provided text doesn't directly compare \"chat\" and \"generate_content\". It focuses on the capabilities of the `generate_content` method. Here's what we can infer:\n","> \n","> * **`generate_content` is versatile:** It can handle various tasks, including multi-turn chat and even processing multimodal inputs (like text and images).  However, this depends on the capabilities of the underlying model.\n","> * **Current limitations:** The available models only support text and image inputs, and text outputs. This implies that while `generate_content` *could* handle chat, the current models might not be fully optimized for it. \n","> \n","> **In summary:**\n","> \n","> We can't definitively say how \"chat\" and \"generate_content\" differ based on this text. However, it shows that `generate_content` is a flexible method for generating content, including potential chat functionality, but its current implementation is limited to text and image input/output.  We need more information to understand the specific differences between \"chat\" and \"generate_content\". \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","question= \"What is the difference between chat and generate context?\"\n","excerpt= \"\"\" The generate_content method can handle a wide variety\n","of use cases, including multi-turn chat and multimodal input,\n","depending on what the underlying model supports. The available\n","models only support text and images as input, and text as output.\n","In the simplest case, you can pass a prompt string to the\n","GenerativeModel.generate_content method:\n","\"\"\"\n","prompt = question + excerpt\n","response = chat.send_message(prompt)\n","\n","to_markdown(response.text)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":5175,"status":"ok","timestamp":1717533710084,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"JLkgYrC_a4I3","outputId":"c6cbe67d-7cfc-4743-d533-7269f72aea7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 0 ns\n","Wall time: 1.1 s\n"]},{"data":{"text/markdown":["> The chat so far has focused on understanding the differences between \"chat\" and \"generate_content\" in the context of the Gemini framework. We learned:\n","> \n","> * **Chat:** This functionality enables free-flowing conversations, managing the conversation state automatically.\n","> * **Generate Content:** This functionality is more versatile, potentially handling multi-turn chat and multimodal inputs. However, current models only support text and image input/output, limiting its chat capabilities. \n","> \n","> We've been trying to pinpoint the specific differences between these two functionalities, but the provided text doesn't explicitly define them. We need more information to get a complete picture. \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","question= \"Summarize the chat so far:\"\n","excerpt= \"\"\n","prompt = question + excerpt\n","response = chat.send_message(prompt)\n","\n","to_markdown(response.text)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"executionInfo":{"elapsed":3693,"status":"ok","timestamp":1717533742120,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"PZQac5D_bT8B","outputId":"2695d16b-f5ba-4684-bec9-70924b68c0de"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 0 ns\n","Wall time: 632 ms\n"]},{"data":{"text/markdown":["> I don't know.  The provided text excerpts don't mention anything about streaming chat.  We would need additional information about the Gemini framework or chat functionality to answer this question. \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","question= \"How to stream the chat?\"\n","excerpt= \"\"\n","prompt = question + excerpt\n","response = chat.send_message(prompt)\n","\n","to_markdown(response.text)"]},{"cell_type":"markdown","metadata":{"id":"fcithWncPNyk"},"source":["## How Many Tokens --> How much does it cost?\n","\n","https://ai.google.dev/pricing"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":3791,"status":"ok","timestamp":1717533831495,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"TUaFWjL6Eb2e","outputId":"7d8ebecc-d1e8-44b6-f43f-2f248080b2e2"},"outputs":[{"data":{"text/plain":["total_tokens: 1034"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model.count_tokens(chat.history)"]},{"cell_type":"markdown","metadata":{"id":"HZDbzRVbC9cE"},"source":["## Build a simple Interface with Gradio"]},{"cell_type":"code","execution_count":26,"metadata":{"collapsed":true,"id":"TaIYCRb-Xndl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gradio in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.42.0)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (23.2.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (4.4.0)\n","Requirement already satisfied: fastapi in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.112.1)\n","Requirement already satisfied: ffmpy in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.4.0)\n","Requirement already satisfied: gradio-client==1.3.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (1.3.0)\n","Requirement already satisfied: httpx>=0.24.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.27.0)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.24.6)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (6.4.4)\n","Requirement already satisfied: jinja2<4.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (3.9.2)\n","Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (3.10.7)\n","Requirement already satisfied: packaging in c:\\users\\ecoskun\\appdata\\roaming\\python\\python311\\site-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (10.4.0)\n","Requirement already satisfied: pydantic>=2.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (2.8.2)\n","Requirement already satisfied: pydub in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.0.9)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (6.0.2)\n","Requirement already satisfied: ruff>=0.2.2 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.6.2)\n","Requirement already satisfied: semantic-version~=2.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (2.10.0)\n","Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.12.0)\n","Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.12.4)\n","Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\ecoskun\\appdata\\roaming\\python\\python311\\site-packages (from gradio) (4.12.2)\n","Requirement already satisfied: urllib3~=2.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (2.2.2)\n","Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio) (0.30.6)\n","Requirement already satisfied: fsspec in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n","Requirement already satisfied: websockets<13.0,>=10.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gradio-client==1.3.0->gradio) (12.0)\n","Requirement already satisfied: idna>=2.8 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n","Requirement already satisfied: sniffio>=1.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: certifi in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n","Requirement already satisfied: httpcore==1.* in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n","Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n","Requirement already satisfied: requests in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ecoskun\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.0->gradio) (2.20.1)\n","Requirement already satisfied: click>=8.0.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n","Requirement already satisfied: starlette<0.39.0,>=0.37.2 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi->gradio) (0.38.2)\n","Requirement already satisfied: colorama in c:\\users\\ecoskun\\appdata\\roaming\\python\\python311\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n","Requirement already satisfied: six>=1.5 in c:\\users\\ecoskun\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ecoskun\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\ecoskun\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"]}],"source":["!pip install gradio"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jULTsJpLXlH9"},"outputs":[],"source":["import gradio as gr"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"5ocGWoudYBPz"},"outputs":[],"source":["def build_chatBot(system_instruction):\n","  model = genai.GenerativeModel('gemini-1.5-flash-latest', system_instruction=system_instruction)\n","  chat = model.start_chat(history=[])\n","  return chat\n"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"1L4M4v_TXmUR"},"outputs":[],"source":["def chat_with_gemini(prompt, context, chat):\n","  response = chat.send_message(\" Question: \"+ prompt + \" Context: \"+ context)\n","  '''\n","  # Format the chat history for display\n","  formatted_history = \"\\n \".join(\n","        f\"{item.role.capitalize()}: {item.parts if hasattr(item, 'parts') else item.content} \"\n","        for item in chat.history\n","  )\n","  formatted_history = formatted_history.replace(\"[text: \", \"\").replace(\"]\", \"\")\n","  return formatted_history\n","\n","  '''\n","  return response.text\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"s9oMtDawx2A5"},"outputs":[],"source":["def chat_interface(prompt, context):\n","    response = chat_with_gemini(prompt, context, chat)\n","    return response"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"uoOn6YTzpE5X"},"outputs":[],"source":["system_prompt= \"\"\" You are an attentive and supportive academic assistant.\n","           Your task is to provide assistance based solely on the provided\n","           context. I will provide you the question and related text.\n","           Answer the following questions, ensuring your responses\n","           are derived exclusively from the provided partial texts.\n","           If the answer cannot be found within the provided context,\n","           kindly respond with 'I don't know'.\n","           After answering each question, please provide a detailed\n","           explanation, breaking down the answer step by step and relating\n","           it to the provided context.\n","           If you are ready, I will provide you the question and context.\n","        \"\"\""]},{"cell_type":"code","execution_count":32,"metadata":{"id":"FzGLkjjuX_AN"},"outputs":[],"source":["chat = build_chatBot(system_prompt)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"qZ8WNRDCq8JX"},"outputs":[],"source":["prompt=\"What is FC?\"\n","context= \"\"\"FC lets developers create a description\n","of a F in their code, then pass that description to a language\n","model in a request. The response from the model includes the name of\n","a F that matches the description and the arguments to call it with.\n","FC lets you use F as tools in generative AI applications,\n","and you can define more than one F within a single request.\n","\"\"\""]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"executionInfo":{"elapsed":5124,"status":"ok","timestamp":1717527651509,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"M_XvbuLwYobL","outputId":"23e38f73-8804-4619-9704-e1d4073906d3"},"outputs":[{"data":{"text/markdown":["> I don't know. \n","> \n","> The provided text defines what FC is and what it does but does not mention what \"F\" stands for. Therefore, it is impossible to determine what FC is based on the provided text. \n"],"text/plain":["<IPython.core.display.Markdown object>"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["response=chat_with_gemini(prompt, context,chat)\n","to_markdown(response)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"PLjp8NReZNm2"},"outputs":[],"source":["demo = gr.Interface(\n","    fn=chat_interface,\n","    inputs=[\n","        gr.Textbox(label=\"Prompt\", value=prompt),  # Label the prompt input\n","        gr.Textbox(label=\"Context\", value=context)  # Label the excerpt input\n","    ],\n","    outputs=\"markdown\",  # Specify output as markdown\n","    title=\"Chat with Gemini\",\n","    description=\"Type your question with the context to chat with the Gemini model.\"\n",")\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":645},"executionInfo":{"elapsed":497876,"status":"ok","timestamp":1717534684105,"user":{"displayName":"murat karakaya","userId":"13131480982289717426"},"user_tz":-180},"id":"S0PWcvrJZeJa","outputId":"13504d1f-77aa-4e90-d0f0-0989b0b60b77"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7860\n","\n","Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["demo.launch(share=True, debug=True)"]},{"cell_type":"markdown","metadata":{"id":"_bF_bx48Z6VF"},"source":["# END OF PART A\n","# A SHORT INTRO GEMINI API FOR TEXT GENERATION & CHAT\n","\n","https://ai.google.dev/gemini-api/docs\n","\n","### In this tutorial, we covered:\n","* The Python SDK for the Gemini API\n","* Check the Google LLM Models available via the provided API\n","* Interact with the models using 2 Alternative Interfaces\n","  1. Generate text interface\n","  2. Interact with the models using Multi-turn conversations (chat) interface\n","\n","* Understand Model & Chat objects\n","  * Model Object in detail\n","  * System Prompt in the Gemini API\n","  * Chat Object in detail\n","* Chat using system_instruction: ***A Manual RAG?***\n","* How Many Tokens --> How much does it cost?\n","* Build a simple Interface with Gradio\n","\n","# In the next tutorial, we will cover ChromaDB as a building block for a RAG pipeline!\n","\n","* Stay tuned!"]}],"metadata":{"colab":{"provenance":[{"file_id":"1EwjmwQ7uT4O57vfJEsvP0fL_knhYKPrm","timestamp":1724682313134},{"file_id":"1W3YF8f3-HWbUsU0hsF4xZ6ytL5fIe4wb","timestamp":1717423427220},{"file_id":"1Y1j4Y8BkG9hGa62UelKgLdf9JKUQlNo_","timestamp":1712146481551}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
