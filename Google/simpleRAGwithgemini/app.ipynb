{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ecoskun\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001-tuning\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/chat-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 Chat (Legacy)',\n",
       "       description='A legacy text-only model optimized for chat conversations',\n",
       "       input_token_limit=4096,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "       temperature=0.25,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/text-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 (Legacy)',\n",
       "       description='A legacy model that understands text and generates text as an output',\n",
       "       input_token_limit=8196,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "       temperature=0.7,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Latest',\n",
       "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
       "                    'model.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
       "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
       "                    'model that supports tuning.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-vision-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-pro-vision',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-1.5-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro Latest',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro 001',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-exp-0801',\n",
       "       base_model_id='',\n",
       "       version='exp-0801',\n",
       "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash Latest',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 001',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-001-tuning',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=16384,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [m for m in genai.list_models()]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_content => without memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Unfortunately, I cannot give you an exact number of ways to access a model in the Gemini API. This is because:\n",
       "> \n",
       "> * **Gemini API is not publicly available.** Google hasn't released an official public API for Gemini models. \n",
       "> * **The API structure is constantly evolving.** As Google develops the Gemini API, the access methods and details are likely to change.\n",
       "> \n",
       "> **However, I can tell you about general approaches used for accessing language models through APIs:**\n",
       "> \n",
       "> 1. **REST API:** This is a common approach where you send requests to an API endpoint using HTTP methods like GET, POST, PUT, DELETE. You would include parameters in the request to specify the model, input text, and desired response format. \n",
       "> \n",
       "> 2. **gRPC API:** This is another option for communication. It uses a binary protocol for efficient communication, often used for high-performance applications.\n",
       "> \n",
       "> 3. **Client Libraries:** Many APIs offer client libraries in different programming languages (e.g., Python, JavaScript, Java) that simplify the process of making API calls. \n",
       "> \n",
       "> 4. **Cloud Services:** Some language model providers offer their models through cloud services, making them accessible through platforms like Google Cloud, AWS, or Azure. \n",
       "> \n",
       "> **To learn about accessing Gemini models, you should:**\n",
       "> \n",
       "> * **Follow Google's official announcements:** Keep an eye on Google's blog, news, and developer resources for any updates on Gemini API release or beta access.\n",
       "> * **Join Google's developer forums or communities:**  This can be a great place to find information, ask questions, and connect with other developers.\n",
       "> \n",
       "> Remember that the information above provides a general overview.  The specific details and methods of accessing Gemini models will depend on the API structure Google releases. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(\"How many different ways to acccess a model in Gemini API?\") #generate_content hafiza tutmuyor\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, I cannot give you an exact number of ways to access a model in the Gemini API. This is because:\\n\\n* **Gemini API is not publicly available.** Google hasn't released an official public API for Gemini models. \\n* **The API structure is constantly evolving.** As Google develops the Gemini API, the access methods and details are likely to change.\\n\\n**However, I can tell you about general approaches used for accessing language models through APIs:**\\n\\n1. **REST API:** This is a common approach where you send requests to an API endpoint using HTTP methods like GET, POST, PUT, DELETE. You would include parameters in the request to specify the model, input text, and desired response format. \\n\\n2. **gRPC API:** This is another option for communication. It uses a binary protocol for efficient communication, often used for high-performance applications.\\n\\n3. **Client Libraries:** Many APIs offer client libraries in different programming languages (e.g., Python, JavaScript, Java) that simplify the process of making API calls. \\n\\n4. **Cloud Services:** Some language model providers offer their models through cloud services, making them accessible through platforms like Google Cloud, AWS, or Azure. \\n\\n**To learn about accessing Gemini models, you should:**\\n\\n* **Follow Google's official announcements:** Keep an eye on Google's blog, news, and developer resources for any updates on Gemini API release or beta access.\\n* **Join Google's developer forums or communities:**  This can be a great place to find information, ask questions, and connect with other developers.\\n\\nRemember that the information above provides a general overview.  The specific details and methods of accessing Gemini models will depend on the API structure Google releases. \\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Unfortunately, I cannot give you an exact number of ways to access a model in the Gemini API. This is because:\\n\\n* **Gemini API is not publicly available.** Google hasn't released an official public API for Gemini models. \\n* **The API structure is constantly evolving.** As Google develops the Gemini API, the access methods and details are likely to change.\\n\\n**However, I can tell you about general approaches used for accessing language models through APIs:**\\n\\n1. **REST API:** This is a common approach where you send requests to an API endpoint using HTTP methods like GET, POST, PUT, DELETE. You would include parameters in the request to specify the model, input text, and desired response format. \\n\\n2. **gRPC API:** This is another option for communication. It uses a binary protocol for efficient communication, often used for high-performance applications.\\n\\n3. **Client Libraries:** Many APIs offer client libraries in different programming languages (e.g., Python, JavaScript, Java) that simplify the process of making API calls. \\n\\n4. **Cloud Services:** Some language model providers offer their models through cloud services, making them accessible through platforms like Google Cloud, AWS, or Azure. \\n\\n**To learn about accessing Gemini models, you should:**\\n\\n* **Follow Google's official announcements:** Keep an eye on Google's blog, news, and developer resources for any updates on Gemini API release or beta access.\\n* **Join Google's developer forums or communities:**  This can be a great place to find information, ask questions, and connect with other developers.\\n\\nRemember that the information above provides a general overview.  The specific details and methods of accessing Gemini models will depend on the API structure Google releases. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 14,\n",
       "        \"candidates_token_count\": 358,\n",
       "        \"total_token_count\": 372\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Please provide me with more context! I need to know what you're referring to. For example, did you ask me about a specific API? Or did you ask me to do something that would use an API? \n",
       "> \n",
       "> Tell me more about what you're looking for, and I'll be happy to help. ðŸ˜Š \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate_content(\"Which API did I ask you?\")\n",
    "to_markdown(response.text)# memory yok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Unfortunately, I need more information to answer your question accurately. There's no single, definitive answer to \"how many ways to access a Gemini model\" because it depends on several factors, including:\n",
       "> \n",
       "> * **What you mean by \"access\"**: Do you want to:\n",
       ">     * **Use it directly via an API call**?\n",
       ">     * **Utilize it through a specific platform or service** (e.g., Google Search, Bard, etc.)?\n",
       ">     * **Access it from a third-party application or library** that integrates with the Gemini API?\n",
       "> * **The specific Gemini model you're targeting**: Google has released multiple Gemini models (e.g., Gemini Pro, Gemini Ultra) with varying capabilities and access methods.\n",
       "> \n",
       "> **Here's a breakdown of common ways to access Gemini models:**\n",
       "> \n",
       "> * **Direct API access:** This is likely the most flexible and granular way to interact with Gemini models. You'd use the Google Cloud AI Platform API to send requests and receive responses, allowing you to customize the input and output formats.\n",
       "> * **Integrated platforms and services:** Google offers various services that leverage Gemini models, like:\n",
       ">     * **Google Search:** Gemini Pro is integrated into Google Search, influencing its results and providing richer information.\n",
       ">     * **Bard:** Google's AI chatbot, Bard, is powered by Gemini Pro, enabling it to generate creative text, translate languages, and answer questions.\n",
       ">     * **Other Google products:** Gemini models may be integrated into other Google products, like Google Assistant or Google Docs, in the future.\n",
       "> * **Third-party applications and libraries:** Some developers create applications and libraries that leverage the Gemini API, offering user-friendly interfaces and specific functionalities tailored to their needs.\n",
       "> \n",
       "> **To provide a specific answer, please clarify your question:**\n",
       "> \n",
       "> * **What do you mean by \"accessing\" a Gemini model?**\n",
       "> * **Which specific Gemini model are you interested in?**\n",
       "> \n",
       "> Once you provide this information, I can give you a more accurate and detailed response. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "#response = model.generate_content(\"How many different ways to acccess a model in the Gemini API?\")\n",
    "chat = model.start_chat(history=[])\n",
    "response = chat.send_message(\"How many different ways to acccess a model in the Gemini API?\")\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> You haven't asked me about any specific API. We were discussing the different ways to access Gemini models, which are part of Google's AI platform. \n",
       "> \n",
       "> Perhaps you're thinking of a different API you've interacted with? Could you please provide more context or tell me what API you're referring to? \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =chat.send_message(\"Which API did I ask you?\")\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash-latest',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
