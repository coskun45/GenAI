{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env dosyasını yükle\n",
    "load_dotenv()\n",
    "\n",
    "# .env dosyasındaki anahtarı al\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, sys\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "exchangerate_api_key=os.getenv(\"EXCHANGE_API_KEY\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD to TRY rate: 34.1324\n",
      "TRY to USD rate: 0.0293\n"
     ]
    }
   ],
   "source": [
    "url = f'https://v6.exchangerate-api.com/v6/{exchangerate_api_key}/latest/USD'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "usd_to_tl_rate = data['conversion_rates']['TRY']\n",
    "print(f\"USD to TRY rate: {usd_to_tl_rate}\")\n",
    "\n",
    "url = f'https://v6.exchangerate-api.com/v6/{exchangerate_api_key}/latest/TRY'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "usd_to_tl_rate = data['conversion_rates']['USD']\n",
    "print(f\"TRY to USD rate: {usd_to_tl_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_cost(distance:float):\n",
    "    \"\"\"Calculates delivery cost based on distance in KMs.\"\"\"\n",
    "    return distance*10\n",
    "\n",
    "def tax(amount:float):\n",
    "    \"\"\"Calculates tax based on purchased price.\"\"\"\n",
    "    return amount*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_to_LLM =[delivery_cost,tax]\n",
    "functions_selected_byLLM = {\"delivery_cost\": delivery_cost, \"tax\":tax}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[function_declarations {\n",
       "   name: \"delivery_cost\"\n",
       "   description: \"Calculates delivery cost based on distance in KMs.\"\n",
       "   parameters {\n",
       "     type_: OBJECT\n",
       "     properties {\n",
       "       key: \"distance\"\n",
       "       value {\n",
       "         type_: NUMBER\n",
       "       }\n",
       "     }\n",
       "     required: \"distance\"\n",
       "   }\n",
       " }\n",
       " function_declarations {\n",
       "   name: \"tax\"\n",
       "   description: \"Calculates tax based on purchased price.\"\n",
       "   parameters {\n",
       "     type_: OBJECT\n",
       "     properties {\n",
       "       key: \"amount\"\n",
       "       value {\n",
       "         type_: NUMBER\n",
       "       }\n",
       "     }\n",
       "     required: \"amount\"\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash-latest',tools=functions_to_LLM)\n",
    "model._tools.to_proto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[content {\n",
       "  parts {\n",
       "    text: \"Okay, and what is the price of the TV? I need to know the price to calculate the tax. \\n\"\n",
       "  }\n",
       "  role: \"model\"\n",
       "}\n",
       "finish_reason: STOP\n",
       "index: 0\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HATE_SPEECH\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HARASSMENT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = model.start_chat()\n",
    "response = chat.send_message('I bought a tv.')\n",
    "response.candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[content {\n",
       "  parts {\n",
       "    function_call {\n",
       "      name: \"tax\"\n",
       "      args {\n",
       "        fields {\n",
       "          key: \"amount\"\n",
       "          value {\n",
       "            number_value: 100\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  role: \"model\"\n",
       "}\n",
       "finish_reason: STOP\n",
       "index: 0\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HARASSMENT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HATE_SPEECH\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('I bought a tv for 100 Dollar.')\n",
    "response.candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function name: 'tax' argument name and value: 'amount=100.0'\n"
     ]
    }
   ],
   "source": [
    "#print out each of the funcition calls requested from this single call\n",
    "for part in response.parts:\n",
    "    if fn :=part.function_call:\n",
    "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "        print(f\"function name: '{fn.name}' argument name and value: '{args}'\")\n",
    "    else:\n",
    "        print(\"No function Call found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'tax' called and the result is: 150.0\n"
     ]
    }
   ],
   "source": [
    "# call the selected function with the decided argument and value\n",
    "for part in response.parts:\n",
    "    if fn :=part.function_call:\n",
    "        func_name = fn.name\n",
    "        args = fn.args\n",
    "\n",
    "        if func_name in functions_selected_byLLM:\n",
    "            result = functions_selected_byLLM[func_name](**args)\n",
    "            print(f\"Function '{func_name}' called and the result is: {result}\")\n",
    "    else:\n",
    "        print(\"No function Call found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATM demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, sys\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "exchangerate_api_key=os.getenv(\"EXCHANGE_API_KEY\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = 5000\n",
    "def deposit(amount:float):\n",
    "    \"\"\"Adds the given amout to the balance.\"\"\"\n",
    "    global balance\n",
    "    if amount>0:\n",
    "        balance+=amount\n",
    "        msg=f\"Successfully deposited {amount} TL. Your new balance is {balance} TL.\"\n",
    "    else:\n",
    "        msg=\"Invalid deposti amount. Please enter a positive amount\"\n",
    "    return msg\n",
    "\n",
    "def wihtdraw(amount:float):\n",
    "    \"\"\"Withdraws the given amount from the balance if sufficient funds are available.\"\"\"\n",
    "    global balance\n",
    "    if amount>0 and amount<=balance:\n",
    "        balance-=amount\n",
    "        msg=f\"Successfully withdrew {amount} TL. Your new balance is {balance} TL.\"\n",
    "    elif amount>balance:\n",
    "        msg=\"Insufficient funds. Your current balance is {balance} TL.\"\n",
    "    else:\n",
    "        msg=\"Invalid withdrawal amount. Please enter a positive amount.\"\n",
    "    return msg\n",
    "\n",
    "def get_exchange_rates(from_currency:str,to_currency:str):\n",
    "    \"\"\"Display the exchange rates from USD to TRY of from TRY to USD.\"\"\"\n",
    "    url = f'https://v6.exchangerate-api.com/v6/{exchangerate_api_key}/latest/{from_currency}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    rate = data['conversion_rates'][to_currency]\n",
    "    msg = f\"{from_currency} to {to_currency} rate: {rate}\"\n",
    "    return msg\n",
    "\n",
    "def exit_from_chat():\n",
    "    \"\"\"Prints a goodbye message and ends the interaction.\"\"\"\n",
    "    print(\"Thank you for using our bank services.\")\n",
    "    return \"Goodbye!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_to_LLM=[deposit,wihtdraw,get_exchange_rates,exit_from_chat]\n",
    "functions_selected_by_LLM = {\n",
    "    \"deposit\": deposit,\n",
    "    \"withdraw\":wihtdraw,\n",
    "    \"get_exchange_rates\":get_exchange_rates,\n",
    "    \"exit_from_chat\":exit_from_chat\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a system instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction= \"\"\"\n",
    "Your are a multi-lingual bank assistant for an ATM system.\n",
    "You can speak not only in English but any language the user interacts with you.\n",
    "\n",
    "The customer's account is denominated in Turkish liras (TRY).\n",
    "\n",
    "Your goal is to guide customers through the following banking functions:\n",
    "1. Withdraw money (in TRY).\n",
    "2. Deposit money (in TRY).  \n",
    "3. Show exchange rates between USD and TRY (both directions).\n",
    "4. Exit the chat.  \n",
    "\n",
    "\n",
    "### Guidelilnes\n",
    "- Focus only on the four banking operations listed above. Do not offer assistance with anything else or any other banking operations.  \n",
    "- for each customer interaction, predict which banking functio is needed based on their input and initiate the relevant operation.  \n",
    "- Always confirm the transaction and the amount with the user before function call.  \n",
    "- After withdreal or deposti transaction provide  the updated account balance in TRY.  \n",
    "- Greet the user warmly at the start and guide them politely throughourt the interaction. \n",
    "- when the user wishes to end the session, choose the \"exit\" function to conclude the chat.  \n",
    "\n",
    "### Language Instructions:\n",
    "- Primarily communicate in English, but if the customer responds in another language (e.g Turkish), continue the conversation in that language (Turkish).  \n",
    "- Switch the language according to the language user in the last user respond.\n",
    "- Before replying anything, check the use languagen and switch the language according to it.  \n",
    "\n",
    "Remember, your focus is to assist with these specific banking functions.  Make sure to correctly infer the user's intent based on their reponses and offer the appropriate service.  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_op(fn):\n",
    "    if fn :=part.function_call:\n",
    "        func_name=fn.name\n",
    "        args = fn.args  \n",
    "\n",
    "        if func_name in functions_selected_by_LLM:\n",
    "            result = functions_selected_by_LLM[func_name](**args)\n",
    "            return result\n",
    "        else:\n",
    "            return f\"Function {func_name} not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_chat(system_instruction,functions_to_LLM):\n",
    "    generation_config = {\n",
    "        \"temperature\":0.3,\n",
    "        \"top_p\":0.95,\n",
    "        \"top_k\":65,\n",
    "        \"max_output_tokens\":8192,\n",
    "    }\n",
    "    model = genai.GenerativeModel(model_name='gemini-1.5-flash-latest',\n",
    "                                  generation_config=generation_config,\n",
    "                                  safety_settings={\n",
    "                                      HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,},\n",
    "                                      tools =functions_to_LLM,\n",
    "                                      system_instruction=system_instruction)\n",
    "    chat = model.start_chat()\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input: 'content' argument must not be empty. Please provide a non-empty value.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m user_request\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mparts:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fn\u001b[38;5;241m:=\u001b[39mpart\u001b[38;5;241m.\u001b[39mfunction_call:          \u001b[38;5;66;03m# LLM decides calling a function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ecoskun\\AppData\\Local\\anaconda3\\envs\\genAIenv\\Lib\\site-packages\\google\\generativeai\\generative_models.py:564\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[1;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported configuration: The `google.generativeai` SDK currently does not support the combination of `stream=True` and `enable_automatic_function_calling=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    562\u001b[0m tools_lib \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_get_tools_lib(tools)\n\u001b[1;32m--> 564\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content\u001b[38;5;241m.\u001b[39mrole:\n\u001b[0;32m    567\u001b[0m     content\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m=\u001b[39m _USER_ROLE\n",
      "File \u001b[1;32mc:\\Users\\ecoskun\\AppData\\Local\\anaconda3\\envs\\genAIenv\\Lib\\site-packages\\google\\generativeai\\types\\content_types.py:271\u001b[0m, in \u001b[0;36mto_content\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_content\u001b[39m(content: ContentType):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content:\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument must not be empty. Please provide a non-empty value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m         )\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, Mapping):\n\u001b[0;32m    276\u001b[0m         content \u001b[38;5;241m=\u001b[39m _convert_dict(content)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid input: 'content' argument must not be empty. Please provide a non-empty value."
     ]
    }
   ],
   "source": [
    "chat = init_chat(system_instruction,functions_to_LLM)\n",
    "user_request=\"Hello\"\n",
    "while True:\n",
    "    response = chat.send_message(user_request)\n",
    "    for part in response.parts:\n",
    "        if fn:=part.function_call:          # LLM decides calling a function\n",
    "            runction_return = execute_op(fn)\n",
    "            response_part = genai.protos.Part(\n",
    "                function_response =genai.protos.FunctionResponse(\n",
    "                    name=fn.name,\n",
    "                    response={\"result\": runction_return}))\n",
    "            response = chat.send_message(response_part)\n",
    "            clerk_response=response.text\n",
    "            user_request=input(clerk_response)\n",
    "        elif clerk_response:=part.text: # LLM decides replying\n",
    "            user_request=input(clerk_response)\n",
    "    if \"goodbye!\" in clerk_response.lower():\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
